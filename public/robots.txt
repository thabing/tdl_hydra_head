# See http://www.robotstxt.org/wc/norobots.html for documentation on how to use the robots.txt file
#
# To ban all spiders from the entire site uncomment the next two lines:
#User-Agent: *
#Disallow: /
User-agent: YandexBot
Disallow: / # blocks access to the entire site

User-agent: Yandex
Disallow: / # blocks access to the entire site

User-agent: Sosospider
Disallow: /

User-agent: Baiduspider
Disallow: /

User-agent: Baiduspider/2.0
Disallow: /

User-agent: Baiduspider+
Disallow: /

User-agent: 008
Disallow: /

User-agent: Deepnet Explorer
Disallow: /

User-agent: Deepnet Explorer 1.5.0
Disallow: /

User-agent: bingbot
Crawl-delay: 10

User-agent: msnbot 
Crawl-delay: 10

User-agent: googlebot-image
Disallow: /

User-agent: Slurp
Disallow: /

User-agent: Teoma
Disallow: /

User-agent: baiduspider
Disallow: /

User-agent: naverbot
Disallow: /

User-agent: yeti
Disallow: /

User-agent: yahoo-mmcrawler
Disallow: /

User-agent: psbot
Disallow: /

User-agent: asterias
Disallow: /

User-agent: yahoo-blogs/v3.9
Disallow: /

User-agent: *
Crawl-delay: 120
Disallow: /cgi-bin/
Disallow: /view*
Disallow: /xslt*
